{
 "metadata": {
  "name": "",
  "signature": "sha256:4e47b44762cce8297cf44c9670405238521b442ab77f9345e2ac71744c3b5d18"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import dot\n",
      "from numpy.linalg import norm\n",
      "\n",
      "from spacy.en import English\n",
      "parser = English()\n",
      "\n",
      "# you can access known words from the parser's vocabulary\n",
      "# nasa = parser.vocab['nasa']\n",
      "man = parser.vocab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from spacy.en import English\n",
      "parser = English()\n",
      "\n",
      "nasa = parser.vocab[u'apple']\n",
      "\n",
      "# # cosine similarity\n",
      "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
      "\n",
      "# # gather all known words, take only the lowercased versions\n",
      "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"india\")})\n",
      "\n",
      "# # sort by similarity to NASA\n",
      "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
      "allWords.reverse()\n",
      "print(\"Top 20 most similar words to NASA:\")\n",
      "for word in allWords[:20]:   \n",
      "    print(word.orth_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 20 most similar words to NASA:\n",
        "lol\n",
        "haha\n",
        "lmao\n",
        "hahaha\n",
        "yeah\n",
        "wtf\n",
        "btw\n",
        "anyways\n",
        "cuz\n",
        "omg\n",
        "hehe\n",
        "yep\n",
        "dude\n",
        ":p\n",
        "idk\n",
        "yea\n",
        "huh\n",
        "yup\n",
        "shit\n",
        "hey\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    \n",
      "# # Let's see if it can figure out this analogy\n",
      "# Man is to King as Woman is to ??\n",
      "king = parser.vocab[u'king']\n",
      "man = parser.vocab[u'man']\n",
      "woman = parser.vocab[u'woman']\n",
      "\n",
      "result = king.vector - man.vector + woman.vector\n",
      "\n",
      "# # gather all known words, take only the lowercased versions\n",
      "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"king\") and w.lower_ != unicode(\"man\") and w.lower_ != unicode(\"woman\")})\n",
      "# # sort by similarity to the result\n",
      "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
      "allWords.reverse()\n",
      "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
      "for word in allWords[:3]:   \n",
      "    print(word.orth_)\n",
      "    \n",
      "# # it got it! Queen!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 20 most similar words to NASA:\n",
        "consistency\n",
        "hotter\n",
        "patches\n",
        "journey\n",
        "featured\n",
        "ref\n",
        "puppies\n",
        "artwork\n",
        "crystal\n",
        "defenses\n",
        "helmet\n",
        "slowed\n",
        "promotion\n",
        "similarities\n",
        "boner\n",
        "attending\n",
        "flex\n",
        "flew\n",
        "adapter\n",
        "acquire\n",
        "\n",
        "----------------------------\n",
        "Top 3 closest results for king - man + woman:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "queen\n",
        "kings\n",
        "princess\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
      "from sklearn.metrics import accuracy_score\n",
      "# from nltk.corpus import stopwords\n",
      "import string\n",
      "import re\n",
      "\n",
      "STOPLIST = [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS)\n",
      "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"\u201c\", \"\u201d\", \"'ve\"]\n",
      "\n",
      "class CleanTextTransformer(TransformerMixin):\n",
      "    def transform(self, X, **transform_params):\n",
      "        return [cleanText(text) for text in X]\n",
      "\n",
      "    def fit(self, X, y=None, **fit_params):\n",
      "        return self\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "    \n",
      "def cleanText(text):\n",
      "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
      "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
      "    text = mentionFinder.sub(\"@MENTION\", text)\n",
      "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
      "    text = text.lower()\n",
      "    return text\n",
      "\n",
      "def tokenizeText(sample):\n",
      "    tokens = parser(sample)\n",
      "    lemmas = []\n",
      "    for tok in tokens:\n",
      "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
      "    tokens = lemmas\n",
      "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
      "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
      "    while \"\" in tokens:\n",
      "        tokens.remove(\"\")\n",
      "    while \" \" in tokens:\n",
      "        tokens.remove(\" \")\n",
      "    while \"\\n\" in tokens:\n",
      "        tokens.remove(\"\\n\")\n",
      "    while \"\\n\\n\" in tokens:\n",
      "        tokens.remove(\"\\n\\n\")\n",
      "    return tokens\n",
      "\n",
      "def printNMostInformative(vectorizer, clf, N):\n",
      "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
      "    feature_names = vectorizer.get_feature_names()\n",
      "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
      "    topClass1 = coefs_with_fns[:N]\n",
      "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
      "    print(\"Class 1 best: \")\n",
      "    for feat in topClass1:\n",
      "        print(feat)\n",
      "    print(\"Class 2 best: \")\n",
      "    for feat in topClass2:\n",
      "        print(feat)\n",
      "\n",
      "# the vectorizer and classifer to use\n",
      "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
      "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
      "clf = LinearSVC()\n",
      "# the pipeline to clean, tokenize, vectorize, and classify\n",
      "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
      "\n",
      "# data\n",
      "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \"lol @twitterdude that is gr8\", \n",
      "        \"twitter &amp; reddit are fun.\", \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \"Rockets launch from Earth and go to other planets.\",\n",
      "        \"twitter social media &gt; &lt;\", \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
      "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
      "\n",
      "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
      "labelsTest = [\"twitter\", \"space\"]\n",
      "\n",
      "# train\n",
      "pipe.fit(train, labelsTrain)\n",
      "\n",
      "# test\n",
      "preds = pipe.predict(test)\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"results:\")\n",
      "for (sample, pred) in zip(test, preds):\n",
      "    print(sample, \":\", pred)\n",
      "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
      "\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"Top 10 features used to predict: \")\n",
      "# show the top features\n",
      "printNMostInformative(vectorizer, clf, 10)\n",
      "\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
      "# let's see what the pipeline was transforming the data into\n",
      "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
      "transform = pipe.fit_transform(train, labelsTrain)\n",
      "\n",
      "# get the features that the vectorizer learned (its vocabulary)\n",
      "vocab = vectorizer.get_feature_names()\n",
      "\n",
      "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
      "for i in range(len(train)):\n",
      "    s = \"\"\n",
      "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
      "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
      "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
      "        s += str((vocab[idx], num))\n",
      "    print(\"Sample {}: {}\".format(i, s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'parser' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-6d483787e45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/shivam/.local/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/shivam/.local/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/shivam/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/shivam/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/shivam/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-5-6d483787e45d>\u001b[0m in \u001b[0;36mtokenizeText\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizeText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'parser' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction stop_words import ENGLISH_STOP_WORDS as stopwords\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "import string\n",
      "punctuations = string.punctuation\n",
      "\n",
      "# Custom transformer using spaCy\n",
      "class predictors(TransformerMixin):\n",
      "    def transform(self, X, **transform_params):\n",
      "        return [clean_text(text) for text in X]\n",
      "\n",
      "    def fit(self, X, y=None, **fit_params):\n",
      "        return self\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "\n",
      "# Basic utility function to clean the text\n",
      "def clean_text(text):\n",
      "    return text.strip().lower()\n",
      "\n",
      "# Create spacy tokenizer that parses a sentence and generates tokens\n",
      "# these can also be replaced by word vectors\n",
      "def spacy_tokenizer(sentence):\n",
      "    tokens = parser(sentence)\n",
      "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
      "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
      "    return tokens\n",
      "\n",
      "# Create vectorizer object to generate feature vectors, we will use custom spacy\u2019s tokenizer\n",
      "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
      "classifier = LinearSVC()\n",
      "\n",
      "# Create the  pipeline to clean, tokenize, vectorize, and classify\n",
      "pipe = Pipeline([(\"cleaner\", predictors()), \n",
      "                 ('vectorizer', vectorizer), \n",
      "                 ('classifier', classifier)])\n",
      "\n",
      "# Load sample data\n",
      "train = [('I love this sandwich.', 'pos'),\n",
      "         ('this is an amazing place!', 'pos'),\n",
      "         ('I feel very good about these beers.', 'pos'),\n",
      "         ('this is my best work.', 'pos'),\n",
      "         (\"what an awesome view\", 'pos'),\n",
      "         ('I do not like this restaurant', 'neg'),\n",
      "         ('I am tired of this stuff.', 'neg'),\n",
      "         (\"I can't deal with this\", 'neg'),\n",
      "         ('he is my sworn enemy!', 'neg'),\n",
      "         ('my boss is horrible.', 'neg')]\n",
      "test =  [('the beer was good.', 'pos'),\n",
      "         ('I do not enjoy my job', 'neg'),\n",
      "         (\"I ain't feeling dandy today.\", 'neg'),\n",
      "         (\"I feel amazing!\", 'pos'),\n",
      "         ('Gary is a good friend of mine.', 'pos'),\n",
      "         (\"I can't believe I'm doing this.\", 'neg')]\n",
      "\n",
      "# Create model and measure accuracy\n",
      "pipe.fit([x[0] for x in train], [x[1] for x in train])\n",
      "pred_data = pipe.predict([x[0] for x in test])\n",
      "for (sample, pred) in zip(test, pred_data):\n",
      "    print sample, pred\n",
      "print \"Accuracy:\", accuracy_score([x[1] for x in test], pred_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "cannot import name puncutation",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-076361773d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpuncutation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: cannot import name puncutation"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}