{
 "metadata": {
  "name": "",
  "signature": "sha256:fe912219263f07ddcc791ec8aefe022664390d563ed21a7261056a934fda5470"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import dot\n",
      "from numpy.linalg import norm\n",
      "\n",
      "from spacy.en import English\n",
      "parser = English()\n",
      "\n",
      "# you can access known words from the parser's vocabulary\n",
      "# nasa = parser.vocab['nasa']\n",
      "man = parser.vocab\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "<spacy.lexeme.Lexeme at 0x7ff3596ef820>"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nasa = parser.vocab[u'lol']\n",
      "\n",
      "\n",
      "# # cosine similarity\n",
      "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
      "\n",
      "# # gather all known words, take only the lowercased versions\n",
      "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"india\")})\n",
      "\n",
      "# # sort by similarity to NASA\n",
      "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
      "allWords.reverse()\n",
      "print(\"Top 20 most similar words to NASA:\")\n",
      "for word in allWords[:20]:   \n",
      "    print(word.orth_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 20 most similar words to NASA:\n",
        "lol\n",
        "haha\n",
        "lmao\n",
        "hahaha\n",
        "yeah\n",
        "wtf\n",
        "btw\n",
        "anyways\n",
        "cuz\n",
        "omg\n",
        "hehe\n",
        "yep\n",
        "dude\n",
        ":p\n",
        "idk\n",
        "yea\n",
        "huh\n",
        "yup\n",
        "shit\n",
        "hey\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    \n",
      "# # Let's see if it can figure out this analogy\n",
      "# Man is to King as Woman is to ??\n",
      "king = parser.vocab[u'king']\n",
      "man = parser.vocab[u'man']\n",
      "woman = parser.vocab[u'woman']\n",
      "\n",
      "result = king.vector - man.vector + woman.vector\n",
      "\n",
      "# # gather all known words, take only the lowercased versions\n",
      "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(\"king\") and w.lower_ != unicode(\"man\") and w.lower_ != unicode(\"woman\")})\n",
      "# # sort by similarity to the result\n",
      "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
      "allWords.reverse()\n",
      "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
      "for word in allWords[:3]:   \n",
      "    print(word.orth_)\n",
      "    \n",
      "# # it got it! Queen!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 20 most similar words to NASA:\n",
        "consistency\n",
        "hotter\n",
        "patches\n",
        "journey\n",
        "featured\n",
        "ref\n",
        "puppies\n",
        "artwork\n",
        "crystal\n",
        "defenses\n",
        "helmet\n",
        "slowed\n",
        "promotion\n",
        "similarities\n",
        "boner\n",
        "attending\n",
        "flex\n",
        "flew\n",
        "adapter\n",
        "acquire\n",
        "\n",
        "----------------------------\n",
        "Top 3 closest results for king - man + woman:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "queen\n",
        "kings\n",
        "princess\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
      "from sklearn.metrics import accuracy_score\n",
      "# from nltk.corpus import stopwords\n",
      "import string\n",
      "import re\n",
      "\n",
      "STOPLIST = [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS)\n",
      "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"\u201c\", \"\u201d\", \"'ve\"]\n",
      "\n",
      "class CleanTextTransformer(TransformerMixin):\n",
      "    def transform(self, X, **transform_params):\n",
      "        return [cleanText(text) for text in X]\n",
      "\n",
      "    def fit(self, X, y=None, **fit_params):\n",
      "        return self\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        return {}\n",
      "    \n",
      "def cleanText(text):\n",
      "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
      "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
      "    text = mentionFinder.sub(\"@MENTION\", text)\n",
      "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
      "    text = text.lower()\n",
      "    return text\n",
      "\n",
      "def tokenizeText(sample):\n",
      "    tokens = parser(sample)\n",
      "    lemmas = []\n",
      "    for tok in tokens:\n",
      "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
      "    tokens = lemmas\n",
      "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
      "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
      "    while \"\" in tokens:\n",
      "        tokens.remove(\"\")\n",
      "    while \" \" in tokens:\n",
      "        tokens.remove(\" \")\n",
      "    while \"\\n\" in tokens:\n",
      "        tokens.remove(\"\\n\")\n",
      "    while \"\\n\\n\" in tokens:\n",
      "        tokens.remove(\"\\n\\n\")\n",
      "    return tokens\n",
      "\n",
      "def printNMostInformative(vectorizer, clf, N):\n",
      "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
      "    feature_names = vectorizer.get_feature_names()\n",
      "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
      "    topClass1 = coefs_with_fns[:N]\n",
      "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
      "    print(\"Class 1 best: \")\n",
      "    for feat in topClass1:\n",
      "        print(feat)\n",
      "    print(\"Class 2 best: \")\n",
      "    for feat in topClass2:\n",
      "        print(feat)\n",
      "\n",
      "# the vectorizer and classifer to use\n",
      "# note that I changed the tokenizer in CountVectorizer to use a custom function using spaCy's tokenizer\n",
      "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
      "clf = LinearSVC()\n",
      "# the pipeline to clean, tokenize, vectorize, and classify\n",
      "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
      "\n",
      "# data\n",
      "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \"lol @twitterdude that is gr8\", \n",
      "        \"twitter &amp; reddit are fun.\", \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \"Rockets launch from Earth and go to other planets.\",\n",
      "        \"twitter social media &gt; &lt;\", \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
      "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
      "\n",
      "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
      "labelsTest = [\"twitter\", \"space\"]\n",
      "\n",
      "# train\n",
      "pipe.fit(train, labelsTrain)\n",
      "\n",
      "# test\n",
      "preds = pipe.predict(test)\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"results:\")\n",
      "for (sample, pred) in zip(test, preds):\n",
      "    print(sample, \":\", pred)\n",
      "print(\"accuracy:\", accuracy_score(labelsTest, preds))\n",
      "\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"Top 10 features used to predict: \")\n",
      "# show the top features\n",
      "printNMostInformative(vectorizer, clf, 10)\n",
      "\n",
      "print(\"----------------------------------------------------------------------------------------------\")\n",
      "print(\"The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\")\n",
      "# let's see what the pipeline was transforming the data into\n",
      "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
      "transform = pipe.fit_transform(train, labelsTrain)\n",
      "\n",
      "# get the features that the vectorizer learned (its vocabulary)\n",
      "vocab = vectorizer.get_feature_names()\n",
      "\n",
      "# the values from the vectorizer transformed data (each item is a row,column index with value as # times occuring in the sample, stored as a sparse matrix)\n",
      "for i in range(len(train)):\n",
      "    s = \"\"\n",
      "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
      "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
      "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
      "        s += str((vocab[idx], num))\n",
      "    print(\"Sample {}: {}\".format(i, s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------------------------------------------------------------------------------------\n",
        "results:\n",
        "('i h8 riting comprehensibly #skoolsux', ':', 'twitter')\n",
        "('planets and stars and rockets and stuff', ':', 'space')\n",
        "('accuracy:', 1.0)\n",
        "----------------------------------------------------------------------------------------------\n",
        "Top 10 features used to predict: \n",
        "Class 1 best: \n",
        "(-0.52882868500098756, u'planet')\n",
        "(-0.35193680464693639, u'space')\n",
        "(-0.21829853774137772, u'mar')\n",
        "(-0.21829853774137772, u'red')\n",
        "(-0.1559279900899416, u'earth')\n",
        "(-0.1559279900899416, u'launch')\n",
        "(-0.1559279900899416, u'rocket')\n",
        "(-0.14828074758382351, u'great')\n",
        "(-0.14828074758382351, u'love')\n",
        "(-0.099226847690378761, u'blue')\n",
        "Class 2 best: \n",
        "(0.4112990687747069, u'twitter')\n",
        "(0.34038584206353562, u'@mention')\n",
        "(0.23401527169621708, u'lol')\n",
        "(0.23401527169621708, u'gr8')\n",
        "(0.20564963218448548, u'reddit')\n",
        "(0.20564963218448548, u'fun')\n",
        "(0.20564943659022139, u'social')\n",
        "(0.20564943659022139, u'medium')\n",
        "(0.10637057036731855, u'y')\n",
        "(0.10637057036731855, u'window')\n",
        "----------------------------------------------------------------------------------------------\n",
        "The original data as it appeared to the classifier after tokenizing, lemmatizing, stoplisting, etc\n",
        "Sample 0: (u'great', 1)(u'space', 2)(u'love', 1)\n",
        "Sample 1: (u'exist', 1)(u'glad', 1)(u'cool', 1)(u'planet', 1)(u'space', 1)\n",
        "Sample 2: (u'gr8', 1)(u'@mention', 1)(u'lol', 1)\n",
        "Sample 3: (u'fun', 1)(u'reddit', 1)(u'twitter', 1)\n",
        "Sample 4: (u'red', 1)(u'mar', 1)(u'planet', 1)\n",
        "Sample 5: (u'9', 1)(u'window', 1)(u'skip', 1)(u'u', 1)(u'y', 1)(u'@mention', 1)\n",
        "Sample 6: (u'earth', 1)(u'launch', 1)(u'rocket', 1)(u'planet', 1)\n",
        "Sample 7: (u'medium', 1)(u'social', 1)(u'twitter', 1)\n",
        "Sample 8: (u'hashtag', 1)(u'@mention', 3)\n",
        "Sample 9: (u'green', 1)(u'blue', 1)(u'little', 1)(u'sun', 1)(u'orbit', 1)(u'planet', 1)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:39: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}